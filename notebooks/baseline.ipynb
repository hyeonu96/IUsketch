{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IU Sketch Baseline 코드  \n",
    " - python모듈 설치 코드는 처음 한번 실행해주세요.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imageio\n",
    "# !pip install imageio --upgrade\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 path 설정.\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(os.getcwd()  , '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이스 라인 코드.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import ThresholdedReLU\n",
    "\n",
    "\n",
    "import apps.config_env as cfg\n",
    "\n",
    "from classes.image_frame import ImgFrame\n",
    "from classes.video_clip import VideoClip\n",
    "from models.dataset_generator import DataSetGenerator\n",
    "# from models.layer_conv import Conv2Plus1D, TConv2Plus1D\n",
    "# from models.layer_encoder import Encoder5D, Decoder5D\n",
    "# from models.layer_lstm import ConvLstmSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 디렉토리 없으면 생성.\n",
    "\n",
    "# 학습용 raw_clip(gif) 파일 위치.\n",
    "if not os.path.exists(cfg.RAW_CLIP_PATH):\n",
    "    os.mkdir(cfg.RAW_CLIP_PATH)\n",
    "\n",
    "# 모델 저장 위치\n",
    "if not os.path.exists(cfg.MODEL_SAVE_PATH):\n",
    "    os.mkdir(cfg.MODEL_SAVE_PATH)\n",
    "\n",
    "# 임시 데이터 저장 위치\n",
    "if not os.path.exists(cfg.TEMP_DATA_PATH):\n",
    "    os.mkdir(cfg.TEMP_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.Model):\n",
    "    '''\n",
    "        5-rank Spatial Convolution\n",
    "    '''\n",
    "    def __init__(self, filters, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "            (b, t, h, w, c) -> (b, t, h/stride, w/stride, f)\n",
    "            A sequence of convolutional layers that first apply the convolution operation over the\n",
    "            spatial dimensions, and then the temporal dimension. \n",
    "        \"\"\"\n",
    "        super(Conv2Plus1D, self).__init__()\n",
    "\n",
    "        # Spatial decomposition\n",
    "        self.conv1 = layers.Conv3D(filters=filters,\n",
    "                    kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                    strides=(1, stride, stride),\n",
    "                    padding=padding)\n",
    "\n",
    "        # Temporal decomposition\n",
    "        self.conv2 = layers.Conv3D(filters=filters,\n",
    "                        kernel_size=(kernel_size[0], 1, 1),\n",
    "                        strides=1,\n",
    "                        padding=padding)\n",
    "\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.bn(x)\n",
    "        # x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TConv2Plus1D(keras.Model):\n",
    "    '''\n",
    "        5-rank Spatial TransposeConvolution\n",
    "    '''\n",
    "    def __init__(self, filters, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "            (b, t, h, w, c) -> (b, t, h*stride, w*stride, f)\n",
    "            A sequence of convolutional layers that first apply the convolution operation over the\n",
    "            spatial dimensions, and then the temporal dimension. \n",
    "        \"\"\"\n",
    "        super(TConv2Plus1D, self).__init__()\n",
    "\n",
    "        # Spatial decomposition\n",
    "        self.tconv1 = layers.Conv3DTranspose(filters=filters,\n",
    "                    kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                    strides=(1, stride, stride),\n",
    "                    padding=padding)\n",
    "\n",
    "        # Temporal decomposition\n",
    "        self.tconv2 = layers.Conv3DTranspose(filters=filters, \n",
    "                    kernel_size=(kernel_size[0], 1, 1),\n",
    "                    strides=1,\n",
    "                    padding=padding)\n",
    "\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.tconv1(x)\n",
    "        # x = self.tconv2(x)\n",
    "        # x = self.bn(x)\n",
    "        # x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder5D(keras.Model):\n",
    "    '''\n",
    "        5-rank Encoder\n",
    "    '''\n",
    "    def __init__(self, kernel_count, filters, kernel_size, stride, padding, out_channel=None):\n",
    "        \"\"\"\n",
    "            conv : (b, t, h, w, c) -> (b, t, h/stride, w/stride, f)\n",
    "            kernel_count만큼 반복.\n",
    "        \"\"\"\n",
    "        super(Encoder5D, self).__init__()\n",
    "        self.kernel_count = kernel_count\n",
    "        self.out_channel = out_channel\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        self.relus = []\n",
    "        \n",
    "        for cnt in range(kernel_count):\n",
    "            if stride == 1:\n",
    "                filter_cnt = filters // (2**cnt)\n",
    "            else:\n",
    "                filter_cnt = filters*2**cnt\n",
    "            self.convs.append(Conv2Plus1D(filter_cnt, kernel_size, stride, padding))\n",
    "            self.bns.append(layers.BatchNormalization())\n",
    "            self.relus.append(layers.ReLU())\n",
    "\n",
    "        if out_channel is not None:\n",
    "            self.out_conv = Conv2Plus1D(out_channel, kernel_size, 1, padding=\"same\")\n",
    "            self.out_bn = layers.BatchNormalization()\n",
    "            self.out_relu = layers.ReLU()\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        for idx in range(self.kernel_count):\n",
    "            x = self.convs[idx](x)\n",
    "            x = self.bns[idx](x)\n",
    "            x = self.relus[idx](x)\n",
    "            \n",
    "        if self.out_channel is not None:\n",
    "            x = self.out_conv(x)\n",
    "            x = self.out_bn(x)\n",
    "            x = self.out_relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder5D(keras.Model):\n",
    "    '''\n",
    "        5-rank Decoder\n",
    "    '''\n",
    "    def __init__(self, kernel_count, filters, kernel_size, stride, padding, out_channel=None):\n",
    "        \"\"\"\n",
    "            (b, t, h, w, c) -> (b, t, h*stride, w*stride, f)\n",
    "            kernel_count만큼 반복.\n",
    "        \"\"\"\n",
    "        super(Decoder5D, self).__init__()\n",
    "        self.kernel_count = kernel_count\n",
    "        self.out_channel = out_channel\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        self.relus = []\n",
    "\n",
    "        for cnt in range(0, kernel_count):\n",
    "            self.convs.append(TConv2Plus1D(filters // (2**cnt), kernel_size, stride, padding))\n",
    "            self.bns.append(layers.BatchNormalization())\n",
    "            self.relus.append(layers.ReLU())\n",
    "\n",
    "        if out_channel is not None:\n",
    "            self.out_conv = Conv2Plus1D(out_channel, kernel_size, 1, padding=\"same\")\n",
    "            self.out_bn = layers.BatchNormalization()\n",
    "            self.out_relu = layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        for idx in range(self.kernel_count):\n",
    "            x = self.convs[idx](x)\n",
    "            x = self.bns[idx](x)\n",
    "            x = self.relus[idx](x)\n",
    "            \n",
    "        if self.out_channel is not None:\n",
    "            x = self.out_conv(x)\n",
    "            x = self.out_bn(x)\n",
    "            x = self.out_relu(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLstmSeries(keras.Model):\n",
    "    '''\n",
    "        5-rank Colvolution LSTM\n",
    "    '''\n",
    "    def __init__(self, filter_cnt, final_filter_cnt, kernel_sizes):\n",
    "        \"\"\"\n",
    "            (b, t, h, w, c) -> (b, t, h/stride, w/stride, f)\n",
    "            A sequence of convolutional layers that first apply the convolution operation over the\n",
    "            spatial dimensions, and then the temporal dimension.\n",
    "        \"\"\"\n",
    "        super(ConvLstmSeries, self).__init__()\n",
    "        self.filter_cnt = filter_cnt\n",
    "        self.final_filter_cnt = final_filter_cnt\n",
    "        self.kernel_cnt = len(kernel_sizes)\n",
    "        self.lstms = []\n",
    "        self.bns = []\n",
    "        self.relus = []\n",
    "        self.out_conv = None\n",
    "\n",
    "        for kernel_size in kernel_sizes:\n",
    "            self.lstms.append(layers.ConvLSTM2D(\n",
    "                                filters=filter_cnt,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=\"same\",\n",
    "                                return_sequences=True,\n",
    "                                activation=\"relu\",\n",
    "                                kernel_regularizer=\"l2\",\n",
    "                                recurrent_regularizer=\"l2\",                            \n",
    "                            ))\n",
    "            self.bns.append(layers.BatchNormalization())\n",
    "            self.relus.append(layers.ReLU())\n",
    "\n",
    "        # 출력의 channel depth를 맞춰주기 위해.\n",
    "        if final_filter_cnt > 0:\n",
    "            self.out_conv = Conv2Plus1D(final_filter_cnt, (1, 3, 3), 1, \"same\")\n",
    "            self.out_bn = layers.BatchNormalization()\n",
    "            self.out_relu = layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        for idx in range(self.kernel_cnt):\n",
    "            x = self.lstms[idx](x)\n",
    "            x = self.bns[idx](x)\n",
    "            x = self.relus[idx](x)\n",
    "\n",
    "        if self.out_conv is not None:\n",
    "            x = self.out_conv(x)\n",
    "            x = self.out_bn(x)\n",
    "            x = self.out_relu(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = 32, 32 #cfg.DATA_IMG_W, cfg.DATA_IMG_H\n",
    "batch_size = 4 #cfg.DATA_BATCH_SIZE\n",
    "time_steps = 7 # cfg.DATA_TIME_STEP\n",
    "\n",
    "# encoder-decoder 모델 사용시\n",
    "is_lstm_model = False\n",
    "is_autoenc_model = False\n",
    "\n",
    "# dataset 설정.\n",
    "data_seq_type = 'all'  # 'all', 'rest', 'arandom', 'aforward', 'forward', 'reverse', 'random'\n",
    "data_label_type = '1step'   # 'all', 'rest', 'same', '1step'\n",
    "stakced = False\n",
    "overlap = False\n",
    "\n",
    "# 전체 raw_clip 랜덤한 이미지 목록을 가져옴.\n",
    "img_list = glob.glob(os.path.join(cfg.RAW_CLIP_PATH, \"*.gif\"))\n",
    "random.shuffle(img_list)\n",
    "\n",
    "# 이미지 목록을 train/validation용으로 9:1로 나눔.\n",
    "train_val_ratio = 0.9\n",
    "train_img_cnt = int(len(img_list) * train_val_ratio)\n",
    "train_img_list = img_list[:train_img_cnt]\n",
    "val_img_list = img_list[train_img_cnt:]\n",
    "\n",
    "# train/validation용 generator를 생성.\n",
    "tdgen = DataSetGenerator(imgs=train_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "                         seq_type=data_seq_type, label_type=data_label_type,\n",
    "                         stacked=stakced, overlap=overlap)\n",
    "\n",
    "vdgen = DataSetGenerator(imgs=val_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "                         seq_type=data_seq_type, label_type=data_label_type,\n",
    "                         stacked=stakced, overlap=overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder - lstms - decoder - retina(0) 모델을 생성.\n",
    "enc_in_filters = 128\n",
    "enc_conv_count = 3\n",
    "lstm_count = 3\n",
    "dec_conv_count = 3\n",
    "retina_conv_count = 3\n",
    "\n",
    "enc_out_filters = enc_in_filters*2**(enc_conv_count-1)\n",
    "dec_in_filters = enc_out_filters // 2\n",
    "retina_in_filters = dec_in_filters // 2**(dec_conv_count)\n",
    "lstm_in_filters = enc_out_filters\n",
    "if is_lstm_model:\n",
    "    lstm_in_filters = 64\n",
    "    \n",
    "encoder = Encoder5D(enc_conv_count, enc_in_filters, (1, 3, 3), 2, \"same\")\n",
    "decoder = Decoder5D(dec_conv_count, dec_in_filters, (1, 3, 3), 2, \"same\")\n",
    "\n",
    "if is_lstm_model:\n",
    "    lstms = ConvLstmSeries(lstm_in_filters, 1, [(5, 5), (3, 3), (1, 1)])\n",
    "else:\n",
    "    lstms = ConvLstmSeries(enc_out_filters, 0, [(5, 5), (3, 3), (1, 1)])\n",
    "\n",
    "retina = Encoder5D(retina_conv_count, retina_in_filters, (1, 3, 3), 1, \"same\", out_channel=None)\n",
    "retina0 = Encoder5D(0, 0, (1, 3, 3), 1, \"same\", out_channel=1)\n",
    "# threshold_relu = ThresholdedReLU(theta=0.5)\n",
    "\n",
    "inputs = layers.Input(shape=(None, img_w, img_h, 1))\n",
    "\n",
    "if is_lstm_model:\n",
    "    x = lstms(inputs)\n",
    "    \n",
    "elif is_autoenc_model:\n",
    "    x = encoder(inputs)\n",
    "    x = decoder(x)\n",
    "    x = retina(x)\n",
    "    x = retina0(x)\n",
    "\n",
    "else:\n",
    "    x = encoder(inputs)\n",
    "    x = lstms(x)\n",
    "    x = decoder(x)\n",
    "    x = retina(x)\n",
    "    x = retina0(x)\n",
    "\n",
    "# x = threshold_relu(x)\n",
    "outputs = x\n",
    "\n",
    "\n",
    "def dice_score_loss(y_true, y_pred):\n",
    "    numerator = 2. * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return tf.reduce_mean(1 - numerator / denominator)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name='sketcher')\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#model.compile(optimizer = keras.optimizers.Adam(1e-4), loss = dice_score_loss)\n",
    "model.compile(optimizer = keras.optimizers.Adam(1e-4), loss = loss)\n",
    "#model.compile(optimizer = keras.optimizers.Adam(1e-4), loss = 'binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, show_shapes=True, expand_nested=True, show_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all(prefix='base_200'):\n",
    "    # model.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"sketcher_{prefix}\"))\n",
    "    encoder.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"enc_{prefix}\"))\n",
    "    lstms.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"lstm_{prefix}\"))\n",
    "    decoder.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"dec_{prefix}\"))\n",
    "    retina.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"retina_{prefix}\"))\n",
    "    retina0.save(os.path.join(cfg.MODEL_SAVE_PATH, f\"retina0_{prefix}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all(prefix='base_200'):\n",
    "#     model = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"sketcher_{prefix}\"))\n",
    "    encoder = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"enc_{prefix}\"))\n",
    "    lstms = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"lstm_{prefix}\"))\n",
    "    decoder = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"dec_{prefix}\"))\n",
    "    retina = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"retina_{prefix}\"))\n",
    "    retina0 = keras.models.load_model(os.path.join(cfg.MODEL_SAVE_PATH, f\"retina0_{prefix}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all(prefix='base_400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.trainable = False\n",
    "# decoder.trainable = False\n",
    "# lstms.trainable = False\n",
    "# retina.trainable = False\n",
    "# retina0.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_cnt = 1\n",
    "steps_per_epoch = 50\n",
    "val_steps = 2\n",
    "\n",
    "history = model.fit(\n",
    "            tdgen,\n",
    "            validation_data = vdgen,\n",
    "            steps_per_epoch = steps_per_epoch,\n",
    "            validation_steps = val_steps,\n",
    "            epochs = epoch_cnt,\n",
    "            batch_size = batch_size,\n",
    "            verbose = 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all(prefix='base_400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arry5d_to_img(arry5d, save_as='', threshold=0.0):\n",
    "    frmimg_cnt = arry5d.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = frmimg_cnt, figsize=(15, 3))\n",
    "\n",
    "    for idx, num in enumerate(range(0, frmimg_cnt)):\n",
    "        frm = ImgFrame(img=arry5d[0][idx][:, :, :], do_norm=False)\n",
    "        if threshold > 0.0:\n",
    "            frm.threshold(threshold=threshold)\n",
    "        img = frm.to_image()\n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset중 하나만 뽑아서 예측에 입력\n",
    "it = iter(vdgen)\n",
    "x, y = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x 이미지 한개 표시\n",
    "arry5d_to_img(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y 이미지 한개 표시.\n",
    "arry5d_to_img(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 예측하여 이미지 표시.\n",
    "in_x = x[:1, :, :, :, :]\n",
    "pred = model.predict(in_x)\n",
    "\n",
    "file_name = os.path.join(cfg.TEMP_DATA_PATH, 'result.gif')\n",
    "arry5d_to_img(pred, save_as=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user가 그린 임의의 그림 예측.\n",
    "# user_file_name = os.path.join(cfg.TEMP_DATA_PATH, 'user_draw.gif')\n",
    "# user_draw = VideoClip(gif_path=user_file_name)\n",
    "# user_draw.resize(img_w, img_h, inplace=True)\n",
    "# arry5d = user_draw.to_array(expand=True)\n",
    "# print(arry5d.shape)\n",
    "# arry5d_to_img(arry5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict(arry5d)\n",
    "\n",
    "# file_name = os.path.join(cfg.TEMP_DATA_PATH, 'result.gif')\n",
    "# arry5d_to_img(pred, save_as=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6debceb626036820d184549e25d059a55b6b8771e25bc8133db281d329c34fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
