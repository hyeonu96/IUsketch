{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(os.getcwd()  , '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from utils.files import dir_path_change\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from classes.image_frame import ImgFrame\n",
    "from classes.video_clip import VideoClip\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "IMG_LOAD_BASE_PATH = '/home/evergrin/tmp/new_100/'\n",
    "IMG_SAVE_BASE_PATH = '/home/evergrin/tmp/new_100_rename/'\n",
    "\n",
    "\n",
    "gif_list = glob.glob(os.path.join(IMG_LOAD_BASE_PATH, \"*.gif\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일명 변경.\n",
    "\n",
    "for idx, gif_file in enumerate(gif_list):\n",
    "    \n",
    "    # rename gif files\n",
    "    new_file = os.path.join(IMG_SAVE_BASE_PATH, f\"lion_{idx+920:04d}.gif\")\n",
    "    # new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "\n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "    vclip.resize(256, 256)\n",
    "    vclip.make_gif(new_file)\n",
    "    \n",
    "    print(new_file)\n",
    "    # os.system(f\"cp {gif_file} {new_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file의 frame count표시\n",
    "\n",
    "for idx, gif_file in enumerate(gif_list[:1]):\n",
    "    \n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif('/home/evergrin/iu/datas/data_set/lion_1210.gif', grayscale=True)\n",
    "\n",
    "    print(gif_file, len(vclip.clips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first / last frame to check\n",
    "\n",
    "for idx, gif_file in enumerate(gif_list):\n",
    "    \n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "\n",
    "    # shwo first / last frame\n",
    "    first_frame = vclip.clips[0]\n",
    "    last2_frame = vclip.clips[-2]\n",
    "    last_frame = vclip.clips[-1]\n",
    "    \n",
    "    # print(gif_file, \":\", np.array_equal(first_frame, last_frame))\n",
    "    first_img = first_frame.to_image()\n",
    "    last2_img = last2_frame.to_image()\n",
    "    last_img = last_frame.to_image()\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 3))\n",
    "    axes[0].imshow(first_img, cmap='gray')\n",
    "    axes[1].imshow(last2_img, cmap='gray')\n",
    "    axes[2].imshow(last_img, cmap='gray')\n",
    "    print(gif_file)\n",
    "    plt.show()\n",
    "    \n",
    "    aaa = input()\n",
    "    if 'q' == aaa:\n",
    "        break\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif split and rearrange.\n",
    "\n",
    "def split_rearrange(gif_file, size=50):\n",
    "\n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "    top_frame = vclip.clips[0]\n",
    "\n",
    "    newclip = vclip.split(dx=size,dy=size, included_top=True)\n",
    "\n",
    "    newclip = newclip.adjacent_clips(included_top=2, seq_type=1, margin=20)\n",
    "    \n",
    "    # newclip = newclip.all_clips(count=50, include_top=False, use_all=True, shuffle='none', overlap=0)\n",
    "\n",
    "    vclip2 = VideoClip(frames=newclip.clips[1:])\n",
    "    newclip = vclip2.stacked_frames_clip(step=1, included_label=False)\n",
    "\n",
    "    new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "\n",
    "    newclip.make_gif(new_file, fps=20)\n",
    "    # print(newclip)\n",
    "    \n",
    "    return newclip\n",
    "    \n",
    "\n",
    "# for idx, gif_file in enumerate(gif_list[0]):\n",
    "#     split_rearrange(gif_file, size=10) # size 5이상.\n",
    "\n",
    "gif_file = gif_list[0]\n",
    "vclip = VideoClip()\n",
    "vclip2 = VideoClip()\n",
    "vclip.load_gif(gif_file, grayscale=True)\n",
    "\n",
    "vclip2.load_frames(vclip.clips[:1])\n",
    "\n",
    "newclip = vclip2.split(dx=5,dy=5, included_top=False)\n",
    "\n",
    "newclip = newclip.adjacent_clips(included_top=0, seq_type=1, margin=20)\n",
    "newclip = newclip.stacked_frames_clip(step=1, included_label=False)\n",
    "    \n",
    "# clip = split_rearrange(gif_list[1], size=5)\n",
    "# print(clip)\n",
    "# imgfrm = clip.clips[-1]\n",
    "\n",
    "new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "newclip.make_gif(new_file, fps=20)\n",
    "# img = imgfrm.to_image(save_file=new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif split and rearrange for IU-Sketch\n",
    "\n",
    "def split_rearrange(gif_file, size=50):\n",
    "\n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "    \n",
    "    vclip.resize(64, 64, True)\n",
    "    vclip.resize(256, 256, True)\n",
    "    \n",
    "    top_frame = vclip.clips[0]\n",
    "\n",
    "    newclip = vclip.split(dx=size,dy=size, included_top=True)\n",
    "\n",
    "    newclip = newclip.adjacent_clips(included_top=2, seq_type=1, margin=20)\n",
    "    \n",
    "    # newclip = newclip.all_clips(count=50, include_top=False, use_all=True, shuffle='none', overlap=0)\n",
    "\n",
    "#     vclip2 = VideoClip(frames=newclip.clips[1:])\n",
    "#     newclip = vclip2.stacked_frames_clip(step=1, included_label=False)\n",
    "\n",
    "    new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "\n",
    "    newclip.resize(64, 64, True)\n",
    "\n",
    "    newclip.make_gif(new_file, fps=20)\n",
    "    # print(newclip)\n",
    "    \n",
    "    return newclip\n",
    "    \n",
    "\n",
    "for idx, gif_file in enumerate(gif_list):\n",
    "    split_rearrange(gif_file, size=4) # size 5이상.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, gif_file in enumerate(gif_list):\n",
    "    \n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "\n",
    "    newclip = vclip.split(dx=50,dy=50, included_top=True)\n",
    "\n",
    "    newclip = newclip.adjacent_clips(included_top=2, seq_type=1)\n",
    "    \n",
    "    stackedclip = newclip.stacked_frames_clip()\n",
    "\n",
    "    new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "    # print(gif_file, \" -> \", new_file)\n",
    "    newclip.make_gif(new_file, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import ThresholdedReLU\n",
    "\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import apps.config_env as cfg\n",
    "\n",
    "from classes.image_frame import ImgFrame\n",
    "from classes.video_clip import VideoClip\n",
    "from models.dataset_generator import DataSetGenerator\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from models.layer_conv import Conv2Plus1D, TConv2Plus1D\n",
    "# from models.layer_encoder import Encoder5D, Decoder5D\n",
    "# from models.layer_lstm import ConvLstmSeries\n",
    "\n",
    "img_w, img_h = 64, 64 #cfg.DATA_IMG_W, cfg.DATA_IMG_H\n",
    "batch_size = 4 #cfg.DATA_BATCH_SIZE\n",
    "time_steps = 20 # cfg.DATA_TIME_STEP\n",
    "enc_blk_count = 5  # 3 - 7\n",
    "disc_blk_count = 3 # \n",
    "EPOCHS = 50\n",
    "\n",
    "enc_filters = [64,128,256,512,512,512,512,512]\n",
    "dec_filters = [512,512,512,512,256,128,64]\n",
    "\n",
    "\n",
    "# dataset 설정.\n",
    "data_seq_type = 'aforward'  # 'all', 'rest', 'arandom', 'aforward', 'forward', 'reverse', 'random'\n",
    "data_label_type = '1step'   # 'all', 'rest', 'same', '1step'\n",
    "stakced = False\n",
    "overlap = False\n",
    "\n",
    "# 전체 raw_clip 랜덤한 이미지 목록을 가져옴.\n",
    "img_list = glob.glob(os.path.join(cfg.RAW_CLIP_PATH, \"*.gif\"))\n",
    "random.shuffle(img_list)\n",
    "\n",
    "# 이미지 목록을 train/validation용으로 9:1로 나눔.\n",
    "train_val_ratio = 0.9\n",
    "train_img_cnt = int(len(img_list) * train_val_ratio)\n",
    "train_img_list = img_list[:train_img_cnt]\n",
    "val_img_list = img_list[train_img_cnt:]\n",
    "\n",
    "# train/validation용 generator를 생성.\n",
    "tdgen = DataSetGenerator(imgs=train_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "                         seq_type=data_seq_type, label_type=data_label_type,\n",
    "                         stacked=stakced, overlap=overlap, fill_box=True, invert=True)\n",
    "\n",
    "vdgen = DataSetGenerator(imgs=val_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "                         seq_type=data_seq_type, label_type=data_label_type,\n",
    "                         stacked=stakced, overlap=overlap)\n",
    "\n",
    "def arry5d_to_img(arry5d, save_as='', threshold=0.0):\n",
    "    frmimg_cnt = arry5d.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = frmimg_cnt, figsize=(15, 3))\n",
    "\n",
    "    for idx, num in enumerate(range(0, frmimg_cnt)):\n",
    "        frm = ImgFrame(img=arry5d[0][idx][:, :, :], do_norm=False)\n",
    "        min_val = np.min(frm.arry)\n",
    "        max_val = np.max(frm.arry)\n",
    "        frm.arry = (frm.arry - min_val) / (max_val - min_val)\n",
    "\n",
    "        min_val = np.min(frm.arry)\n",
    "        max_val = np.max(frm.arry)\n",
    "        # print(\"min,max: \", np.min(frm.arry), np.max(frm.arry))\n",
    "\n",
    "        if threshold > 0.0:\n",
    "            frm.threshold(threshold=threshold)\n",
    "\n",
    "        img = frm.to_image()\n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def show_imgs(arry1, arry2):\n",
    "    frmimg_cnt = arry1.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = frmimg_cnt, figsize=(15, 3))\n",
    "    arrys = [arry1, arry2]\n",
    "    \n",
    "    for i, arry in enumerate(arrys):\n",
    "        for idx, num in enumerate(range(0, frmimg_cnt)):\n",
    "            frm = ImgFrame(img=arry[0][idx][:, :, :], do_norm=False)\n",
    "            min_val = np.min(frm.arry)\n",
    "            max_val = np.max(frm.arry)\n",
    "            frm.arry = (frm.arry - min_val) / (max_val - min_val)\n",
    "\n",
    "            min_val = np.min(frm.arry)\n",
    "            max_val = np.max(frm.arry)\n",
    "            # print(\"min,max: \", np.min(frm.arry), np.max(frm.arry))\n",
    "\n",
    "            img = frm.to_image()\n",
    "            axes[i][idx].imshow(img, cmap='gray')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(tdgen)\n",
    "x, y = next(it)\n",
    "\n",
    "vclip = VideoClip()\n",
    "vclip.from_array(arry_4d = x[0, ...], do_norm=False)\n",
    "stacked_vclip = vclip.stacked_frames_clip()\n",
    "\n",
    "ary = stacked_vclip.to_array(expand=True)\n",
    "arry5d_to_img(ary[:, :5], threshold=0.)\n",
    "print(ary.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arry5d_to_img(x[:, :5], threshold=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset중 하나만 뽑아서 예측에 입력\n",
    "it = iter(vdgen)\n",
    "x, y = next(it)\n",
    "\n",
    "clip = VideoClip()\n",
    "clip.from_array(y[0, :, :, :, :], do_norm=False)\n",
    "clip = clip.stacked_frames_clip()\n",
    "arry = clip.to_array(expand=True)\n",
    "\n",
    "# x 이미지 한개 표시\n",
    "arry5d_to_img(arry[: ,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arry5d_to_img(1-arry[: ,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apps.frame_maker import FrameMaker\n",
    "\n",
    "dlg = FrameMaker()\n",
    "dlg.runModal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from utils.files import dir_path_change\n",
    "from classes.image_frame import ImgFrame\n",
    "from classes.video_clip import VideoClip\n",
    "\n",
    "IMG_LOAD_BASE_PATH = '/home/evergrin/iu/datas/imgs/raw_imgs/raw_gif/raw_gif_512_100/'\n",
    "IMG_SAVE_BASE_PATH = '/home/evergrin/iu/datas/imgs/raw_imgs/raw_gif/resize_gif/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# contour은 흰색의 물체를 검출.\n",
    "IMG_DIR = '/home/evergrin/iu/datas/imgs/yahoo'\n",
    "# img = cv2.imread(os.path.join(IMG_DIR, 'img0000.png'))\n",
    "img = cv2.imread('/home/evergrin/iu/datas/imgs/data_set/lion_0187_000.png')\n",
    "imgray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# imgray = np.invert(imgray)\n",
    "\n",
    "print(imgray.shape)\n",
    "ret,thresh = cv2.threshold(imgray,200,255,0)\n",
    "thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "sxs = []\n",
    "sys = []\n",
    "exs = []\n",
    "eys = []\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    x,y,w,h = cv2.boundingRect(contours[i])\n",
    "    sxs.append(x)\n",
    "    sys.append(y)\n",
    "    exs.append(x+w)\n",
    "    eys.append(y+h)\n",
    "\n",
    "sxs = np.array(sxs)\n",
    "sys = np.array(sys)\n",
    "exs = np.array(exs)\n",
    "eys = np.array(eys)\n",
    "\n",
    "#     cv2.drawContours(img, [contours[i]], -1, (0, 0, 255), 2)\n",
    "    # cv2.putText(img, str(i), tuple(contours[i][0][0]), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 0), 1)\n",
    "    # print(i, hierarchy[0][i])\n",
    "\n",
    "sx,sy,ex,ey = np.min(sxs), np.min(sys), np.max(exs), np.max(eys)\n",
    "img = cv2.rectangle(img,(sx,sy),(ex,ey),(0, 0, 255), 2)\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "# # cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE) # cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # \n",
    "\n",
    "# print(hierarchy)\n",
    "\n",
    "# cnt = contours[0]\n",
    "\n",
    "# x,y,w,h = cv2.boundingRect(cnt)\n",
    "# print(x, y, w, h)\n",
    "# imgray = cv2.rectangle(imgray,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "# plt.imshow(thresh, cmap='gray')\n",
    "# cv2.imshow('img', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리에 있는 gif file을 1/2 사이즈로 일괄 변경한다.\n",
    "\n",
    "gif_list = glob.glob(os.path.join(IMG_LOAD_BASE_PATH, \"*.gif\"))\n",
    "\n",
    "for gif_file in gif_list:\n",
    "\n",
    "    vclip = VideoClip()\n",
    "    vclip.load_gif(gif_file, grayscale=True)\n",
    "    new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "    vclip.make_gif(new_file, ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리에 있는 gif file을 1/2 사이즈로 일괄 변경한다.\n",
    "\n",
    "gif_list = glob.glob(os.path.join(IMG_LOAD_BASE_PATH, \"*.gif\"))\n",
    "\n",
    "for idx, gif_file in enumerate(gif_list):\n",
    "\n",
    "    clip = VideoClip(gif_path=gif_file)\n",
    "    \n",
    "    imgfrm = ImgFrame(img=clip.clips[0], do_norm=False)\n",
    "    \n",
    "    # vclip.load_gif(gif_file, grayscale=True)\n",
    "    file_name = os.path.join(IMG_SAVE_BASE_PATH, f\"lion_{500+idx:04d}.png\")\n",
    "    imgfrm.to_image(save_file=file_name)\n",
    "    # new_file = dir_path_change(gif_file, IMG_SAVE_BASE_PATH, \"gif\")\n",
    "    # vclip.make_gif(new_file, ratio=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyenv_3912')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6debceb626036820d184549e25d059a55b6b8771e25bc8133db281d329c34fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
